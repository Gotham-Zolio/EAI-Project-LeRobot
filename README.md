# EAI Course Project: Diffusion Policy for LeRobot SO-101

This repository implements a Diffusion Policy for the SO-101 dual-arm manipulator in SAPIEN simulation, supporting Lift, Stack, and Sort tasks. Developed for Embodied AI 2025 (Track 1).

## ğŸ“ Repository Structure

```
EAI-Project-LeRobot/
â”œâ”€â”€ assets/SO101/              # Robot models (URDF, SRDF, meshes)
â”œâ”€â”€ configs/                   # Hydra configs
â”‚   â”œâ”€â”€ train.yaml            # Training configuration
â”‚   â”œâ”€â”€ env/default.yaml
â”‚   â”œâ”€â”€ policy/diffusion.yaml
â”‚   â””â”€â”€ robots/
â”œâ”€â”€ data/datasets/             # Collected demonstrations
â”‚   â”œâ”€â”€ {task}/
â”‚   â”‚   â”œâ”€â”€ raw/              # HDF5: {task}_{version}_{date}.h5
â”‚   â”‚   â””â”€â”€ meta/             # Metadata: {task}_{version}_info.json
â”œâ”€â”€ logs/                      # Training and evaluation outputs
â”‚   â”œâ”€â”€ train/{task}/{date}/{time}/
â”‚   â”‚   â”œâ”€â”€ checkpoints/      # Model weights
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_10.pth
â”‚   â”‚   â”‚   â”œâ”€â”€ checkpoint_epoch_100.pth
â”‚   â”‚   â”‚   â””â”€â”€ last.pth      # Latest checkpoint
â”‚   â”‚   â”œâ”€â”€ logs/             # TensorBoard logs
â”‚   â”‚   â”œâ”€â”€ eval_results/     # Evaluation metrics
â”‚   â”‚   â””â”€â”€ stats.json        # Normalization + dataset metadata
â”‚   â””â”€â”€ simulation/           # Debug outputs
â”œâ”€â”€ scripts/                   # Main entry points
â”‚   â”œâ”€â”€ collect_data.py       # FSM+IK data collection
â”‚   â”œâ”€â”€ train.py              # Policy training
â”‚   â”œâ”€â”€ eval.py               # Policy evaluation
â”‚   â””â”€â”€ sim_env_demo.py       # Environment sanity check
â”œâ”€â”€ src/lerobot/              # Core implementation
â”‚   â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ policy/
â”‚   â””â”€â”€ real/
â””â”€â”€ tools/                     # Utilities
    â”œâ”€â”€ calibration/
    â””â”€â”€ web_viewer/
```

## ğŸš€ Quick Start

### Installation

```bash
conda create -n lerobot python=3.10
conda activate lerobot
pip install -r requirements.txt
```

### Sanity Check

```bash
python scripts/sim_env_demo.py --task lift
```

## ğŸ“Š Data Collection

### Collect Demonstrations

Uses FSM + IK to generate expert trajectories. Automatically skips failed IK episodes.

```bash
# First batch (creates lift_v1.0_YYYYMMDD.h5)
python scripts/collect_data.py task=lift num_episodes=50 version=v1.0

# Append to same version
python scripts/collect_data.py task=lift num_episodes=50 version=v1.0

# New version (after strategy change)
python scripts/collect_data.py task=lift num_episodes=50 version=v1.1
```

**Parameters:**
- `task`: `lift`, `stack`, or `sort`
- `num_episodes`: Episodes to collect
- `version`: Dataset version (e.g., `v1.0`, `v1.1`)
- `headless`: Disable GUI (default: `True`)
- `web_viewer`: Enable web visualization (default: `False`)

**Output Structure:**

```
data/datasets/{task}/
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ {task}_{version}_{date}.h5          # HDF5 episodes
â””â”€â”€ meta/
    â””â”€â”€ {task}_{version}_info.json          # Metadata (counts, success rate, etc.)
```

**Best Practices:**
- Collect 50-100+ episodes per version
- Use same version when appending
- Increment version after strategy changes (v1.0 â†’ v1.1 â†’ v2.0)

## ğŸ‹ï¸ Training

### Train Policy

```bash
# Auto-detect latest dataset
python scripts/train.py task=lift batch_size=8 epochs=100

# Train with specific dataset version
python scripts/train.py task=lift dataset_version=v1.0 epochs=100

# Train with explicit path
python scripts/train.py task=lift dataset_path=data/datasets/lift/raw/lift_v1.0_20260101.h5
```

**Parameters:**
- `task`: Task name (required)
- `batch_size`: Batch size (default: 8)
- `epochs`: Training epochs (default: 100)
- `lr`: Learning rate (default: 1e-4)
- `save_freq`: Checkpoint frequency (default: 10)
- `dataset_version`: Specify dataset version (e.g., `v1.0`)
- `dataset_path`: Explicit dataset path

**Output Structure:**

```
logs/train/{task}/{date}/{time}/
â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ checkpoint_epoch_10.pth
â”‚   â”œâ”€â”€ checkpoint_epoch_100.pth
â”‚   â””â”€â”€ last.pth                      # Latest checkpoint
â”œâ”€â”€ logs/                             # TensorBoard logs
â”œâ”€â”€ .hydra/                           # Config backup
â””â”€â”€ stats.json                        # Normalization + dataset metadata
```

### Monitor Training

```bash
tensorboard --logdir logs/train/lift --port 6006
```

Access at `http://localhost:6006`

**Logged Metrics:**
- Loss/batch, Loss/epoch, GradientNorm/batch, LearningRate, Hyperparameters

## ğŸ¯ Evaluation

### Evaluate Policy

```bash
# Evaluate latest checkpoint
python scripts/eval.py \
  --checkpoint logs/train/lift/{date}/{time}/checkpoints/last.pth \
  --task lift \
  --num-episodes 20

# Evaluate specific epoch
python scripts/eval.py \
  --checkpoint logs/train/lift/{date}/{time}/checkpoints/checkpoint_epoch_100.pth \
  --task lift \
  --num-episodes 20
```

**Parameters:**
- `--checkpoint`: Path to `.pth` file (required)
- `--task`: Task name (required)
- `--num-episodes`: Episodes to evaluate (default: 10)
- `--device`: `cuda` or `cpu` (default: `cuda`)
- `--headless`: Disable SAPIEN GUI
- `--no-web-viewer`: Disable web visualization
- `--port`: Web viewer port (default: 5000)

### Web Viewer

Automatically starts at `http://localhost:5000` showing camera streams.

### Evaluation Output

Results are saved to `{checkpoint_dir}/eval_results/eval_{task}_{timestamp}.json`

## ğŸ”§ Advanced Usage

### Batch Dataset Exploration

```python
import h5py
with h5py.File("data/datasets/lift/raw/lift_v1.0_20260101.h5", "r") as f:
    print(f"Episodes: {f.attrs['num_episodes']}")
    total_steps = sum(f[ep]['action'].shape[0] for ep in f.keys() if ep.startswith('episode_'))
    print(f"Total steps: {total_steps}")
```

### Comparing Multiple Runs

```bash
python scripts/train.py task=lift dataset_version=v1.0 epochs=50
python scripts/train.py task=lift dataset_version=v1.1 epochs=50
tensorboard --logdir logs/train/lift
```

### GPU Selection

```bash
CUDA_VISIBLE_DEVICES=0 python scripts/train.py task=lift epochs=100
CUDA_VISIBLE_DEVICES=2 python scripts/eval.py --checkpoint ... --task lift
```

## ğŸ§© System Details

### Simulator
- **Framework:** SAPIEN 3.x with PhysX
- **Robot:** SO-101 (6 DOF + 2-finger gripper per arm)
- **Physics:** Static friction=2.0, dynamic friction=2.0, restitution=0.0

### Policy
- **Architecture:** DDPM-based Diffusion Policy (100 diffusion steps)
- **Vision:** ResNet18 encoder
- **Observations:** Joint positions (qpos) + RGB images
- **Actions:** Target joint positions

### Tasks
- **Lift:** Grasp cube and raise above threshold
- **Stack:** Stack one cube on another
- **Sort:** Sort cubes by color using both arms

## ğŸ“¦ Dependencies

Python 3.10, PyTorch 2.7+, SAPIEN 3.x, diffusers, hydra-core, h5py, gymnasium

## ğŸ› Troubleshooting

| Issue | Solution |
|-------|----------|
| GPU OOM | Reduce `batch_size` or use `CUDA_VISIBLE_DEVICES` |
| IK failures | Check gripper targets; visualize with `tools/visualize_gripper_pose.py` |
| stats.json missing | Ensure training completed successfully |
| Web viewer won't load | Check port 5000 availability; use `--port` to change |
| SAPIEN/Vulkan errors | Install NVIDIA drivers (warnings on headless systems are normal) |

## ğŸ‘¥ Team

Guanheng Chen, Zuo Gou, Zhengyang Fan

## ğŸ“„ License

Developed for educational purposes as part of Embodied AI 2025.